syntax = "proto3";

package inference;

service InferenceService {
    rpc Generate(GenerateRequest) returns (GenerateResponse);
    rpc GenerateStream(GenerateRequest) returns (stream GenerateStreamChunk);
}

message GenerateRequest {
    string request_id = 1;
    srting prompt = 2;
    uint32 max_new_tokens = 3;
    float temperature = 4;
    float top_p = 5;
    uint64 seed = 6;
    repeated string stop_sequences = 7;
    string model_id = 8;
}

message UsageStats {
  uint32 prompt_tokens = 1;
  uint32 generated_tokens = 2;
  double ttft_ms = 3;
  double total_latency_ms = 4;
}

message GenerateResponse {
  string request_id = 1;
  string text = 2;
  repeated uint32 token_ids = 3;
  UsageStats usage = 4;
}

message GenerateStreamChunk {
  string request_id = 1;
  oneof payload {
    TokenChunk token = 2;
    FinalChunk final = 3;
  }
}

message TokenChunk {
  uint32 token_id = 1;
  string token_text = 2;
  uint32 token_index = 3;
}

message FinalChunk {
  string full_text = 1;
  repeated uint32 token_ids = 2;
  UsageStats usage = 3;
  string finish_reason = 4;
}